{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NanoChat Inference Demo (Google Colab)\n",
        "\n",
        "This notebook shows how to load the NanoChat speedrun checkpoint (`royam0820/nanochat-speedrun-001`) and generate responses programmatically in Google Colab.\n",
        "\n",
        "**Note:** Make sure to enable GPU in Colab: Runtime â†’ Change runtime type â†’ GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install datasets fastapi uvicorn wandb huggingface-hub tokenizers tiktoken regex psutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone nanochat repository and install\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Install Rust (needed for building the rustbpe extension)\n",
        "os.system(\"curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\")\n",
        "os.environ[\"PATH\"] = os.path.expanduser(\"~/.cargo/bin\") + \":\" + os.environ.get(\"PATH\", \"\")\n",
        "\n",
        "# Clone the repo (or use your fork)\n",
        "repo_url = \"https://github.com/karpathy/nanochat.git\"  # Change this if using a fork\n",
        "repo_dir = \"nanochat\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "    \n",
        "# Add to Python path\n",
        "sys.path.insert(0, repo_dir)\n",
        "\n",
        "# Install the package (this will build the Rust extension)\n",
        "# Note: This may take a few minutes to compile the Rust extension\n",
        "os.system(f\"cd {repo_dir} && pip install -e .\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model weights and tokenizer from Hugging Face using snapshot_download\n",
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Model configuration\n",
        "model_id = \"royam0820/nanochat-speedrun-001\"\n",
        "print(f\"Downloading {model_id}...\")\n",
        "\n",
        "# Use snapshot_download to get all files from the model repo\n",
        "# This downloads everything in one go (model.pt, meta_*.json, tokenizer.pkl if available)\n",
        "snapshot_path = snapshot_download(\n",
        "    repo_id=model_id,\n",
        "    repo_type=\"model\",\n",
        "    local_dir=\"out\",\n",
        "    local_dir_use_symlinks=False,  # Use actual files, not symlinks\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model downloaded to: {snapshot_path}\")\n",
        "\n",
        "# Set up the checkpoint directory structure\n",
        "cache_dir = os.path.expanduser(\"~/.cache/nanochat\")\n",
        "checkpoint_dir = os.path.join(cache_dir, \"chatsft_checkpoints\", \"d20\")\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Copy the model files to the expected location\n",
        "if os.path.exists(\"out/model.pt\"):\n",
        "    shutil.copy(\"out/model.pt\", os.path.join(checkpoint_dir, \"model_000700.pt\"))\n",
        "    print(f\"âœ… Model weights copied to {checkpoint_dir}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"model.pt not found in downloaded snapshot\")\n",
        "\n",
        "if os.path.exists(\"out/meta_000700.json\"):\n",
        "    shutil.copy(\"out/meta_000700.json\", os.path.join(checkpoint_dir, \"meta_000700.json\"))\n",
        "    print(f\"âœ… Metadata copied to {checkpoint_dir}\")\n",
        "else:\n",
        "    print(\"âš ï¸  meta_000700.json not found, but continuing...\")\n",
        "\n",
        "print(f\"\\nModel files set up in {checkpoint_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download or recreate tokenizer\n",
        "# First check if tokenizer was downloaded via snapshot_download, then try hf_hub_download\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Set up the tokenizer directory structure\n",
        "cache_dir = os.path.expanduser(\"~/.cache/nanochat\")\n",
        "tokenizer_dir = os.path.join(cache_dir, \"tokenizer\")\n",
        "os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "\n",
        "model_id = \"royam0820/nanochat-speedrun-001\"\n",
        "tokenizer_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
        "\n",
        "# Check if tokenizer was already downloaded via snapshot_download\n",
        "if os.path.exists(\"out/tokenizer.pkl\"):\n",
        "    print(\"âœ… Tokenizer found in snapshot_download output\")\n",
        "    shutil.copy(\"out/tokenizer.pkl\", tokenizer_path)\n",
        "    print(f\"âœ… Tokenizer copied to {tokenizer_path}\")\n",
        "else:\n",
        "    print(f\"Tokenizer not in snapshot, attempting to download from {model_id}...\")\n",
        "    try:\n",
        "        # Try to download tokenizer.pkl from the repo using hf_hub_download\n",
        "        downloaded_path = hf_hub_download(\n",
        "            repo_id=model_id,\n",
        "            filename=\"tokenizer.pkl\",\n",
        "            repo_type=\"model\",\n",
        "        )\n",
        "        # Copy to the expected location\n",
        "        shutil.copy(downloaded_path, tokenizer_path)\n",
        "        print(f\"âœ… Tokenizer downloaded to {tokenizer_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Tokenizer not found in Hugging Face repo: {e}\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"OPTION 1: Upload your local tokenizer to Hugging Face\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"From your local environment where inference works, run:\")\n",
        "        print(\"  python -m scripts.upload_tokenizer\")\n",
        "        print(\"\\nOr manually:\")\n",
        "        print(\"  from huggingface_hub import HfApi\")\n",
        "        print(\"  api = HfApi()\")\n",
        "        print(f\"  api.upload_file(\")\n",
        "        print(f\"      path_or_fileobj='~/.cache/nanochat/tokenizer/tokenizer.pkl',\")\n",
        "        print(f\"      path_in_repo='tokenizer.pkl',\")\n",
        "        print(f\"      repo_id='{model_id}',\")\n",
        "        print(f\"      repo_type='model'\")\n",
        "        print(f\"  )\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"OPTION 2: Recreate tokenizer in Colab (slower, requires data)\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"This will train a new tokenizer with the same parameters.\")\n",
        "        print(\"Run the next cell to recreate it.\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        raise FileNotFoundError(\n",
        "            f\"Tokenizer not found. Please upload tokenizer.pkl to {model_id} \"\n",
        "            \"or use the next cell to recreate it.\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Recreate Tokenizer (if not in HF repo)\n",
        "\n",
        "If the tokenizer is not available in the Hugging Face repo, you can recreate it here.\n",
        "**Note:** This requires downloading training data and may take 10-20 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate tokenizer (only run this if tokenizer download failed above)\n",
        "# This trains a new tokenizer with the same parameters as the original model\n",
        "# Requires: vocab_size=65536, max_chars=2B (same as speedrun.sh)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from nanochat.tokenizer import RustBPETokenizer\n",
        "from nanochat.common import get_base_dir\n",
        "from nanochat.dataset import parquets_iter_batched\n",
        "\n",
        "# Check if tokenizer already exists\n",
        "cache_dir = os.path.expanduser(\"~/.cache/nanochat\")\n",
        "tokenizer_dir = os.path.join(cache_dir, \"tokenizer\")\n",
        "tokenizer_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
        "\n",
        "if os.path.exists(tokenizer_path):\n",
        "    print(f\"âœ… Tokenizer already exists at {tokenizer_path}\")\n",
        "    print(\"Skipping recreation.\")\n",
        "else:\n",
        "    # Parameters matching the original training (from meta_000700.json: vocab_size=65536)\n",
        "    VOCAB_SIZE = 65536\n",
        "    MAX_CHARS = 2_000_000_000  # 2B characters (same as speedrun.sh)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"RECREATING TOKENIZER\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Vocab size: {VOCAB_SIZE}\")\n",
        "    print(f\"Max chars: {MAX_CHARS:,}\")\n",
        "    print(\"This will download training data and train the tokenizer...\")\n",
        "    print(\"This may take 10-20 minutes.\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Download training data (8 shards for ~2B chars)\n",
        "    print(\"\\nðŸ“¥ Downloading training data...\")\n",
        "    os.system(\"python -m nanochat.dataset -n 8\")\n",
        "\n",
        "    # Text iterator\n",
        "    def text_iterator():\n",
        "        nchars = 0\n",
        "        for batch in parquets_iter_batched(split=\"train\"):\n",
        "            for doc in batch:\n",
        "                doc_text = doc[:10_000]  # doc_cap\n",
        "                nchars += len(doc_text)\n",
        "                yield doc_text\n",
        "                if nchars > MAX_CHARS:\n",
        "                    return\n",
        "\n",
        "    print(\"\\nðŸ”¨ Training tokenizer...\")\n",
        "    text_iter = text_iterator()\n",
        "    tokenizer = RustBPETokenizer.train_from_iterator(text_iter, VOCAB_SIZE)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "    tokenizer.save(tokenizer_dir)\n",
        "\n",
        "    print(f\"\\nâœ… Tokenizer recreated and saved to {tokenizer_path}\")\n",
        "    print(\"\\nðŸ’¡ Tip: Upload this to Hugging Face for faster future use:\")\n",
        "    print(f\"   python -m scripts.upload_tokenizer {tokenizer_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "import torch\n",
        "from nanochat.common import compute_init, autodetect_device_type\n",
        "from nanochat.checkpoint_manager import load_model\n",
        "from nanochat.engine import Engine\n",
        "\n",
        "device_type = autodetect_device_type()\n",
        "_, _, _, _, device = compute_init(device_type)\n",
        "\n",
        "model, tokenizer, meta = load_model(\n",
        "    source=\"sft\",\n",
        "    device=device,\n",
        "    phase=\"eval\",\n",
        "    model_tag=\"d20\",\n",
        "    step=700,\n",
        ")\n",
        "\n",
        "print(f\"Loaded NanoChat model on {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the inference engine\n",
        "from contextlib import nullcontext\n",
        "\n",
        "bos = tokenizer.get_bos_token_id()\n",
        "user_start = tokenizer.encode_special(\"<|user_start|>\")\n",
        "user_end = tokenizer.encode_special(\"<|user_end|>\")\n",
        "assistant_start = tokenizer.encode_special(\"<|assistant_start|>\")\n",
        "assistant_end = tokenizer.encode_special(\"<|assistant_end|>\")\n",
        "\n",
        "engine = Engine(model, tokenizer)\n",
        "\n",
        "if device_type == \"cuda\":\n",
        "    autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16)\n",
        "else:\n",
        "    autocast_ctx = nullcontext()\n",
        "\n",
        "\n",
        "def generate_response(prompt, temperature=0.6, top_k=50, max_tokens=256):\n",
        "    conversation_tokens = [bos]\n",
        "    conversation_tokens.append(user_start)\n",
        "    conversation_tokens.extend(tokenizer.encode(prompt))\n",
        "    conversation_tokens.append(user_end)\n",
        "    conversation_tokens.append(assistant_start)\n",
        "\n",
        "    response_tokens = []\n",
        "    with autocast_ctx:\n",
        "        for token_column, token_masks in engine.generate(\n",
        "            conversation_tokens,\n",
        "            num_samples=1,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "        ):\n",
        "            token = token_column[0]\n",
        "            response_tokens.append(token)\n",
        "            if token == assistant_end:\n",
        "                break\n",
        "\n",
        "    if response_tokens and response_tokens[-1] != assistant_end:\n",
        "        response_tokens.append(assistant_end)\n",
        "\n",
        "    # Decode without the final assistant_end token\n",
        "    text = tokenizer.decode(response_tokens[:-1])\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single prompt example\n",
        "sample_prompt = \"Hello there! What can you do?\"\n",
        "response = generate_response(sample_prompt)\n",
        "print(f\"Prompt: {sample_prompt}\\n\")\n",
        "print(\"NanoChat:\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch prompts example\n",
        "def generate_batch_responses(prompts, temperature=0.6, top_k=50, max_tokens=256):\n",
        "    results = []\n",
        "    for prompt in prompts:\n",
        "        text = generate_response(prompt, temperature=temperature, top_k=top_k, max_tokens=max_tokens)\n",
        "        results.append({\"prompt\": prompt, \"response\": text})\n",
        "    return results\n",
        "\n",
        "batch_prompts = [\n",
        "    \"Explain why the sky appears blue.\",\n",
        "    \"Give me a haiku about autumn.\",\n",
        "    \"What is 7 multiplied by 13?\",\n",
        "]\n",
        "\n",
        "for item in generate_batch_responses(batch_prompts):\n",
        "    print(f\"Prompt: {item['prompt']}\")\n",
        "    print(f\"Response: {item['response']}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
